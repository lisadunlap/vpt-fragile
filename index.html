<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="color-scheme" content="light" />

    <!-- =========
      Edit these fields to customize your paper site.
      ========= -->
    <title>Visually Prompted Benchmarks Are Surprisingly Fragile</title>
    <meta
      name="description"
      content="Project page for “Visually Prompted Benchmarks Are Surprisingly Fragile” (CVPR 2026 submission)."
    />

    <!-- Open Graph (nice previews when sharing) -->
    <meta property="og:title" content="Visually Prompted Benchmarks Are Surprisingly Fragile" />
    <meta
      property="og:description"
      content="Project page."
    />
    <!-- TODO: replace with a real preview image (1200x630 recommended) -->
    <meta property="og:image" content="assets/figures/og-placeholder.png" />

    <link rel="stylesheet" href="styles.css" />
    <link rel="icon" href="assets/favicon/icon.png" />
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js" charset="utf-8"></script>
  </head>

  <body>
    <a class="skip-link" href="#main">Skip to content</a>

    <header class="header" id="top">
      <section class="hero">
        <h1 class="hero__title">Visually Prompted Benchmarks Are Surprisingly Fragile</h1>

        <!-- Authors -->
        <p class="hero__authors">
          <span class="author">Haiwen Feng</span><span class="sep">·</span>
          <span class="author">Long Lian</span><span class="sep">·</span>
          <span class="author">Lisa Dunlap</span><span class="sep">·</span>
          <span class="author">Jiahao Shu</span><span class="sep">·</span>
          <span class="author">XuDong Wang</span><span class="sep">·</span>
          <span class="author">Renhao Wang</span><span class="sep">·</span>
          <span class="author">Trevor Darrell</span><span class="sep">·</span>
          <span class="author">Alane Suhr</span><span class="sep">·</span>
          <span class="author">Angjoo Kanazawa</span>
        </p>
        <p class="hero__affils">UC Berkeley</p>

        <p class="hero__tagline">
          Small, seemingly irrelevant details in visual prompting (marker style, sampling, compression)
          can shift accuracy and reorder VLM leaderboards.
        </p>

        <div class="ctaRow">
          <!-- Replace # with real links when ready -->
          <a class="btn btn--primary" href="#" aria-disabled="true">Paper (PDF)</a>
          <a class="btn btn--ghost" href="#" aria-disabled="true">arXiv</a>
          <a class="btn btn--ghost" href="#" aria-disabled="true">Code</a>
          <a class="btn btn--ghost" href="#" aria-disabled="true">Data</a>
        </div>
      </section>
    </header>

    <main id="main" class="main">
      <section class="section teaser" aria-label="Teaser">
        <div class="container">
          <figure class="mediaCard">
            <img 
              src="assets/figures/vpb_main.png" 
              alt="Visually Prompted Tasks are Fragile: small design changes can shift leaderboards"
              class="video"
              style="width: 100%; display: block; background: #fff;"
            />
            <figcaption class="caption">
              <strong>Figure 1.</strong> Small, seemingly irrelevant changes in visual prompting dramatically alter VLM predictions. Left: Qwen2.5-VL accuracy under different visual marker variants. Right: such variations can reorder entire leaderboards, with model rankings shifting even when nothing about the underlying task changes.
            </figcaption>
          </figure>
        </div>
      </section>

      <section class="section" id="abstract">
        <div class="container">
          <h2 class="h2">Abstract</h2>
          <p class="abstract">
            A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine modern open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in InternVL3-8B ranking alongside or better than much larger models like Gemini 2.5 Pro. Furthermore, we find that even ostensibly irrelevant modeling and inference decisions like JPEG compression can change the model lineup while similar interventions to non-visually prompted tasks have little effect on the results. To address this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. We open-source this benchmark as well as our analysis tools to further facilitate robust evaluation of VLMs.
          </p>
        </div>
      </section>

      <section class="section" id="visually-prompted-tasks">
        <div class="container">
          <h2 class="h2">What are visually prompted tasks?</h2>
          <div class="grid2">
            <div>
              <p class="muted" style="margin-bottom: 1rem; font-size: 16px;">
                Despite rapid progress in vision-language models (VLMs), their visual perception capabilities remain underexplored. <strong>Visual prompting</strong> has emerged as a targeted paradigm: by marking regions in an image with visual markers (such as circles, boxes, or dots) and posing spatial or perceptual questions, these tasks assess low-level visual understanding that humans solve effortlessly—problems we can answer "in a blink." This stands in contrast to the knowledge-centric reasoning required by benchmarks such as MME or MMMU.
              </p>
              <p class="muted" style="font-size: 16px;">
                However, we find that model performance on these visually prompted evaluations is surprisingly sensitive to seemingly minor design choices in the benchmark itself. Variations in the size, style, or layout of visual markers can substantially affect accuracy and even reorder model rankings.
              </p>
            </div>
            <div>
              <figure class="mediaCard">
                <img 
                  src="assets/figures/vpt_tasks_examples.png" 
                  alt="Examples of visually prompted tasks"
                  style="width: 100%; display: block; background: #fff;"
                />
                <figcaption class="caption">
                  <strong>Figure 2.</strong> Examples of visually prompted tasks including relative depth estimation and semantic correspondence.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </section>

      <section class="section" id="results">
        <div class="container">
          <h2 class="h2">Key findings</h2>
          <div class="cards">
            <article class="card">
              <h3 class="h3">Data size matters</h3>
              <p class="muted">
                Random resampling of similarly-sized image subsets can substantially reorder model rankings, even when the subsets are statistically indistinguishable in difficulty. Small benchmark sizes amplify the impact of incidental sampling variation, making it difficult to distinguish genuine differences in model capability from sampling artifacts.
              </p>
            </article>
            <article class="card">
              <h3 class="h3">Marker style matters</h3>
              <p class="muted">
                Seemingly irrelevant design choices—such as switching a red circle to a blue square—can cause accuracy swings of up to 21% on the exact same image-question pairs. These fluctuations are severe enough to completely reshuffle leaderboards, effectively allowing weaker models to rank above stronger ones based solely on marker style.
              </p>
            </article>
            <article class="card">
              <h3 class="h3">Implementation subtleties</h3>
              <p class="muted">
                JPEG compression quality—a factor imperceptible to human evaluators—produces statistically significant changes in accuracy and rankings on visually prompted tasks. Importantly, these effects are specific to visual prompting; applying the same intervention to traditional VLM benchmarks shows negligible impact.
              </p>
            </article>
          </div>

          <h3 class="h2" style="margin-top: 3rem; margin-bottom: 1rem;">Sample size matters</h3>
          <p class="muted" style="margin-bottom: 2rem; line-height: 1.6;">
            Existing visually prompted benchmarks like BLINK are surprisingly small, with only 224 samples for tasks like relative depth estimation and semantic correspondence. Random resampling of image subsets matched in size to BLINK from a larger pool can substantially reorder model rankings, even when the subsets are statistically indistinguishable in difficulty. This finding reveals that small benchmark sizes amplify the impact of incidental sampling variation, making it difficult to distinguish genuine differences in model capability from sampling artifacts.
          </p>

          <div class="figureGrid2" style="margin-bottom: 2rem;">
            <div>
              <img src="assets/figures/DA2k_rank_heatmap_data_splits.png" alt="DA2k ranking heatmap across data splits" style="width: 100%; height: auto;">
              <p class="muted lbCaption" style="margin-top: 10px;">
                <strong>DA2k:</strong> Model rankings across different data splits.
              </p>
            </div>
            <div>
              <img src="assets/figures/SPair-71k_rank_heatmap_data_splits.png" alt="SPair ranking heatmap across data splits" style="width: 100%; height: auto;">
              <p class="muted lbCaption" style="margin-top: 10px;">
                <strong>SPair-71k:</strong> Model rankings across different data splits.
              </p>
            </div>
          </div>

          <h3 class="h2" style="margin-top: 3rem; margin-bottom: 1rem;">Visual Marker Matters</h3>
          <p class="muted" style="margin-bottom: 2rem; line-height: 1.6;">
            We evaluated models across 16 distinct visual marker styles, systematically varying attributes like color, shape, size, and label placement to test robustness. We found that seemingly irrelevant design choices—such as switching a red circle to a blue square—can cause accuracy swings of up to 21% on the exact same image-question pairs. These fluctuations are severe enough to completely reshuffle leaderboards, effectively allowing weaker models to rank above stronger ones based solely on the style of the visual prompt.
          </p>

          <!-- Interactive marker comparison -->
          <div class="interactiveSection" aria-label="Interactive marker comparison">
            <div class="interactiveHeader">
              <h3 class="h3">Interactive marker comparison</h3>
              <p class="muted">
                Select a dataset and marker variant to see how accuracy and rankings change compared to the default.
              </p>
            </div>

            <div class="datasetSelector" style="margin-bottom: 1rem;">
              <label for="datasetSelect" style="font-weight: 600; margin-right: 0.5rem;">Dataset:</label>
              <select id="datasetSelect" class="datasetSelect" style="padding: 0.5rem; border: 1px solid #cbd5e1; border-radius: 0.375rem; font-size: 0.9rem;">
                <option value="DA2k" selected>DA2k</option>
                <option value="SPair">SPair</option>
              </select>
            </div>

            <div class="markerSelector">
              <button class="markerBtn markerBtn--active" data-marker="color_blue">Color: Blue</button>
              <button class="markerBtn" data-marker="marker_square">Marker Type: Square</button>
              <button class="markerBtn" data-marker="radius_3">Radius: 3</button>
              <button class="markerBtn" data-marker="text_offset_below">Text Offset: Below</button>
              <button class="markerBtn" data-marker="font_scale_0.2">Font Scale: 0.2</button>
            </div>

            <div class="interactiveGrid">
              <div class="chartPanel">
                <div id="accuracyChart" class="plotlyChart"></div>
              </div>
              <div class="leaderboardPanel">
                <table class="interactiveLb" id="interactiveLbTable">
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--center">Δ Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody id="interactiveLbBody">
                    <!-- Populated by JS -->
                  </tbody>
                </table>
              </div>
            </div>

            <p class="muted lbCaption">
              <strong>Figure 6 (interactive)</strong>: Accuracy and ranking changes across marker variants.
              Select a dataset and marker above to compare with the default evaluation.
            </p>
          </div>

          <h3 class="h2" style="margin-top: 3rem; margin-bottom: 1rem;">Low level implementation details matter</h3>
          <p class="muted" style="line-height: 1.6; font-size: 16px; max-width: 82ch; margin-bottom: 1.5rem;">
            Even ostensibly irrelevant implementation details can significantly impact model performance on visually prompted tasks. We find that JPEG compression quality—a factor imperceptible to human evaluators—produces statistically significant changes in accuracy and rankings. Importantly, these compression effects are specific to visually prompted benchmarks; applying the same intervention to traditional VLM benchmarks like MMBench shows negligible impact, highlighting the unique fragility of visual prompting evaluations.
          </p>

          <div class="chartPanel" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: 2rem;">
            <img src="assets/figures/jpeg_compression.png" alt="JPEG compression quality effects on model rankings" style="width: 100%; height: auto;">
            <p class="muted lbCaption" style="margin-top: 10px;">
              <strong>Figure 3.</strong> JPEG compression quality can reorder rankings on visually prompted tasks (BLINK RD),
              while rankings on traditional benchmarks (MME) remain comparatively stable.
            </p>
          </div>

          <div class="lbSection" aria-label="Gaming benchmarks by changing markers">
            <div class="interactiveHeader">
              <h3 class="h3">Gaming benchmarks by changing markers</h3>
              <p class="muted">
                We demonstrate that visually prompted leaderboards can be effectively "gamed" by cherry-picking marker styles that favor specific models. For example, simply increasing the font size of a visual marker allowed us to artificially lift a weaker model, InternVL3-8B, to rank higher than the much larger Gemini 2.5 Pro.
              </p>
            </div>

            <div class="lbGrid" role="region" aria-label="Leaderboards under marker variants">
              <section class="lbCard" aria-label="Default leaderboard">
                <div class="lbHead">
                  <div class="lbTitle">Default</div>
                  <div class="lbSub">Standard Evaluation</div>
                </div>
                <table class="interactiveLb">
                  <colgroup>
                    <col class="lbColModel" />
                    <col class="lbColRank" />
                    <col class="lbColScore" />
                  </colgroup>
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Llama 4 Scout</td>
                      <td class="lbCell--center"><span class="lbRank">#1</span></td>
                      <td class="lbCell--right"><span class="lbScore">89.41%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Flash</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Pro</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr class="lbRow lbRow--highlight">
                      <td>InternVL3-<wbr />8B</td>
                      <td class="lbCell--center"><span class="lbRank">#4</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen3-VL-8B</td>
                      <td class="lbCell--center"><span class="lbRank">#4</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4.1</td>
                      <td class="lbCell--center"><span class="lbRank">#6</span></td>
                      <td class="lbCell--right"><span class="lbScore">75.29%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4o</td>
                      <td class="lbCell--center"><span class="lbRank">#7</span></td>
                      <td class="lbCell--right"><span class="lbScore">71.76%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen2.5-VL-7B</td>
                      <td class="lbCell--center"><span class="lbRank">#8</span></td>
                      <td class="lbCell--right"><span class="lbScore">70.59%</span></td>
                    </tr>
                    <tr>
                      <td>Gemma 3-4B</td>
                      <td class="lbCell--center"><span class="lbRank">#9</span></td>
                      <td class="lbCell--right"><span class="lbScore">52.94%</span></td>
                    </tr>
                  </tbody>
                </table>
              </section>

              <section class="lbCard" aria-label="Deflate InternVL3-8B leaderboard">
                <div class="lbHead">
                  <div class="lbTitle">Deflate InternVL3-8B</div>
                  <div class="lbSub">Marker Type Square</div>
                </div>
                <table class="interactiveLb">
                  <colgroup>
                    <col class="lbColModel" />
                    <col class="lbColRank" />
                    <col class="lbColScore" />
                  </colgroup>
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Llama 4 Scout</td>
                      <td class="lbCell--center"><span class="lbRank">#1</span></td>
                      <td class="lbCell--right"><span class="lbScore">90.59%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Pro</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">84.71%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen3-VL-8B</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4.1</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Flash</td>
                      <td class="lbCell--center"><span class="lbRank">#5</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4o</td>
                      <td class="lbCell--center"><span class="lbRank">#6</span></td>
                      <td class="lbCell--right"><span class="lbScore">75.29%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen2.5-VL-7B</td>
                      <td class="lbCell--center"><span class="lbRank">#7</span></td>
                      <td class="lbCell--right"><span class="lbScore">67.06%</span></td>
                    </tr>
                    <tr class="lbRow lbRow--highlight">
                      <td>InternVL3-<wbr />8B</td>
                      <td class="lbCell--center"><span class="lbRank">#8</span></td>
                      <td class="lbCell--right"><span class="lbScore">63.53%</span></td>
                    </tr>
                    <tr>
                      <td>Gemma 3-4B</td>
                      <td class="lbCell--center"><span class="lbRank">#9</span></td>
                      <td class="lbCell--right"><span class="lbScore">55.29%</span></td>
                    </tr>
                  </tbody>
                </table>
              </section>

              <section class="lbCard" aria-label="Inflate InternVL3-8B leaderboard">
                <div class="lbHead">
                  <div class="lbTitle">Inflate InternVL3-8B</div>
                  <div class="lbSub">Font Scale 1.0</div>
                </div>
                <table class="interactiveLb">
                  <colgroup>
                    <col class="lbColModel" />
                    <col class="lbColRank" />
                    <col class="lbColScore" />
                  </colgroup>
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Llama 4 Scout</td>
                      <td class="lbCell--center"><span class="lbRank">#1</span></td>
                      <td class="lbCell--right"><span class="lbScore">85.88%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Flash</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">81.18%</span></td>
                    </tr>
                    <tr class="lbRow lbRow--highlight">
                      <td>InternVL3-<wbr />8B</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen3-VL-8B</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Pro</td>
                      <td class="lbCell--center"><span class="lbRank">#5</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4o</td>
                      <td class="lbCell--center"><span class="lbRank">#6</span></td>
                      <td class="lbCell--right"><span class="lbScore">72.94%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4.1</td>
                      <td class="lbCell--center"><span class="lbRank">#7</span></td>
                      <td class="lbCell--right"><span class="lbScore">71.76%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen2.5-VL-7B</td>
                      <td class="lbCell--center"><span class="lbRank">#8</span></td>
                      <td class="lbCell--right"><span class="lbScore">70.59%</span></td>
                    </tr>
                    <tr>
                      <td>Gemma 3-4B</td>
                      <td class="lbCell--center"><span class="lbRank">#9</span></td>
                      <td class="lbCell--right"><span class="lbScore">49.41%</span></td>
                    </tr>
                  </tbody>
                </table>
              </section>
            </div>

            <p class="muted lbCaption">
              <strong>Figure</strong>: Performance comparison — optimizing for InternVL3-8B's ranking on
              BLINK relative depth by changing the visual marker.
            </p>
          </div>
        </div>
      </section>

      <section class="section" id="citation">
        <div class="container">
          <h2 class="h2">Citation</h2>

          <div class="bibBox">
            <div class="bibBox__header">
              <div class="bibBox__label">BibTeX</div>
              <button class="btn btn--small" type="button" data-copy-target="#bibtex">
                Copy
              </button>
            </div>
            <pre class="bibtex" id="bibtex" style="white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word;"><code>@inproceedings{vptblink2026fragile,
  title     = {Visually Prompted Benchmarks Are Surprisingly Fragile},
  author    = {Haiwen Feng and Long Lian and Lisa Dunlap and Jiahao Shu and XuDong Wang and Renhao Wang and Trevor Darrell and Alane Suhr and Angjoo Kanazawa},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2026},
  note      = {Submission},
}</code></pre>
          </div>
        </div>
      </section>
    </main>

    <script src="script.js"></script>
  </body>
</html>


