<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="color-scheme" content="light" />

    <!-- =========
      Edit these fields to customize your paper site.
      ========= -->
    <title>Visually Prompted Benchmarks Are Surprisingly Fragile</title>
    <meta
      name="description"
      content="Project page for “Visually Prompted Benchmarks Are Surprisingly Fragile” (CVPR 2026 submission)."
    />

    <!-- Open Graph (nice previews when sharing) -->
    <meta property="og:title" content="Visually Prompted Benchmarks Are Surprisingly Fragile" />
    <meta
      property="og:description"
      content="Project page."
    />
    <!-- TODO: replace with a real preview image (1200x630 recommended) -->
    <meta property="og:image" content="assets/figures/og-placeholder.png" />

    <link rel="stylesheet" href="styles.css" />
    <link rel="icon" href="assets/favicon/icon.png" />
    <script src="https://cdn.plot.ly/plotly-2.27.0.min.js" charset="utf-8"></script>
  </head>

  <body>
    <a class="skip-link" href="#main">Skip to content</a>

    <header class="header" id="top">
      <section class="hero">
        <h1 class="hero__title">Visually Prompted Benchmarks Are Surprisingly Fragile</h1>

        <!-- Authors -->
        <p class="hero__authors">
          <span class="author">Haiwen Feng</span><span class="sep">·</span>
          <span class="author">Long Lian</span><span class="sep">·</span>
          <span class="author">Lisa Dunlap</span><span class="sep">·</span>
          <span class="author">Jiahao Shu</span><span class="sep">·</span>
          <span class="author">XuDong Wang</span><span class="sep">·</span>
          <span class="author">Renhao Wang</span><span class="sep">·</span>
          <span class="author">Trevor Darrell</span><span class="sep">·</span>
          <span class="author">Alane Suhr</span><span class="sep">·</span>
          <span class="author">Angjoo Kanazawa</span>
        </p>
        <p class="hero__affils">UC Berkeley</p>

        <p class="hero__tagline">
          Small, seemingly irrelevant details in visual prompting (marker style, sampling, compression)
          can shift accuracy and reorder VLM leaderboards.
        </p>

        <div class="ctaRow">
          <!-- Replace # with real links when ready -->
          <a class="btn btn--primary" href="#" aria-disabled="true">Paper</a>
          <a class="btn btn--ghost" href="#" aria-disabled="true">Code (Coming soon)</a>
          <a class="btn btn--ghost" href="#" aria-disabled="true">Data (Coming soon)</a>
        </div>
      </section>
    </header>

    <main id="main" class="main">
      <section class="section teaser" aria-label="Teaser">
        <div class="container">
          <figure class="mediaCard">
            <img 
              src="assets/figures/vpb_main.png" 
              alt="Visually Prompted Tasks are Fragile: small design changes can shift leaderboards"
              class="video"
              style="width: 100%; display: block; background: #fff;"
            />
            <figcaption class="caption">
              <strong>Small, seemingly irrelevant changes in visual prompting dramatically alter VLM predictions.</strong> Left: Qwen2.5-VL accuracy under different visual marker variants. Right: such variations can reorder entire leaderboards, with model rankings shifting even when nothing about the underlying task changes.
            </figcaption>
          </figure>
        </div>
      </section>

      <section class="section" id="abstract">
        <div class="container">
          <h2 class="h2">Abstract</h2>
          <p class="abstract">
            A key challenge in evaluating VLMs is testing models' ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through <em>visual prompting</em>, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings.
            <br /><br />
            These effects can even be exploited to <strong>lift weaker models above stronger ones</strong>; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations.
            <br /><br />
            To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants.
          </p>
        </div>
      </section>

      <section class="section" id="visually-prompted-tasks">
        <div class="container">
          <h2 class="h2">What are visually prompted tasks?</h2>
          <div class="grid2">
            <div>
              <p class="muted" style="margin-bottom: 1rem; font-size: 16px;">
                Despite rapid progress in vision-language models (VLMs), their visual perception capabilities remain underexplored. <strong>Visual prompting</strong> has emerged as a targeted paradigm: by marking regions in an image with visual markers (such as circles, boxes, or dots) and posing spatial or perceptual questions, these tasks assess low-level visual understanding that humans solve effortlessly—problems we can answer "in a blink." This stands in contrast to the knowledge-centric reasoning required by benchmarks such as MME or MMMU.
              </p>
              <p class="muted" style="font-size: 16px;">
                However, we find that model performance on these visually prompted evaluations is surprisingly sensitive to seemingly minor design choices in the benchmark itself. Variations in the size, style, or layout of visual markers can substantially affect accuracy and even reorder model rankings.
              </p>
            </div>
            <div>
              <figure class="mediaCard">
                <img 
                  src="assets/figures/vpt_tasks_examples.png" 
                  alt="Examples of visually prompted tasks"
                  style="width: 100%; display: block; background: #fff;"
                />
                <figcaption class="caption">
                  <strong>Examples of visually prompted tasks.</strong> Relative depth estimation and semantic correspondence.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </section>

      <section class="section" id="results">
        <div class="container">
          <h2 class="h2">Key findings</h2>
          <div class="cards">
            <article class="card">
              <h3 class="h3">Data size matters</h3>
              <p class="muted">
                Random resampling of similarly-sized image subsets can substantially reorder model rankings, even when the subsets are statistically indistinguishable in difficulty. Small benchmark sizes amplify the impact of incidental sampling variation, making it difficult to distinguish genuine differences in model capability from sampling artifacts.
              </p>
            </article>
            <article class="card">
              <h3 class="h3">Marker style matters</h3>
              <p class="muted">
                Seemingly irrelevant design choices, such as switching a red circle to a blue square, can cause accuracy swings of up to 21% on the exact same image-question pairs. These fluctuations are severe enough to completely reshuffle leaderboards, effectively allowing weaker models to rank above stronger ones based solely on marker style.
              </p>
            </article>
            <article class="card">
              <h3 class="h3">Implementation subtleties</h3>
              <p class="muted">
                JPEG compression quality, a factor imperceptible to human evaluators, produces statistically significant changes in accuracy and rankings on visually prompted tasks. Importantly, these effects are specific to visual prompting; applying the same intervention to traditional VLM benchmarks shows negligible impact.
              </p>
            </article>
          </div>

          <h3 class="h2" style="margin-top: 3rem; margin-bottom: 1rem;">Sample size matters</h3>
          <p class="muted" style="margin-bottom: 2rem; line-height: 1.6;">
            Existing visually prompted benchmarks like BLINK are surprisingly small, with only 224 samples for tasks like relative depth estimation and semantic correspondence. Random resampling of image subsets matched in size to BLINK from a larger pool can substantially reorder model rankings, even when the subsets are statistically indistinguishable in difficulty. This finding reveals that small benchmark sizes amplify the impact of incidental sampling variation, making it difficult to distinguish genuine differences in model capability from sampling artifacts.
          </p>

          <div class="figureGrid2" style="margin-bottom: 0.75rem;">
            <div>
              <img src="assets/figures/DA2k_rank_heatmap_data_splits.png" alt="DA2k ranking heatmap across data splits" style="width: 100%; height: auto;">
            </div>
            <div>
              <img src="assets/figures/SPair-71k_rank_heatmap_data_splits.png" alt="SPair ranking heatmap across data splits" style="width: 100%; height: auto;">
            </div>
          </div>
          <p class="muted lbCaption" style="margin-top: 10px; margin-bottom: 2rem;">
            <strong>Ranking instability across splits.</strong> Model rankings across 1,000 independent 100-sample splits for DA2k and SPair-71k, with the first 10 splits visualized on the x-axis, revealing substantial ranking volatility caused solely by i.i.d. resampling.
          </p>

          <h3 class="h2" style="margin-top: 3rem; margin-bottom: 1rem;">Visual Marker Matters</h3>
          <p class="muted" style="margin-bottom: 2rem; line-height: 1.6;">
            We evaluated models across 16 distinct visual marker styles, systematically varying attributes like color, shape, size, and label placement to test robustness. We found that seemingly irrelevant design choices—such as switching a red circle to a blue square—can cause accuracy swings of up to 21% on the exact same image-question pairs. These fluctuations are severe enough to completely reshuffle leaderboards, effectively allowing weaker models to rank above stronger ones based solely on the style of the visual prompt. To help make this sensitivity measurable (and harder to ignore), we introduce <strong>VPBench</strong>: expanded, marker-diverse versions of existing visually prompted datasets that evaluate each example under multiple marker variants.
          </p>

          <!-- Interactive marker comparison -->
          <div class="interactiveSection" aria-label="Interactive marker comparison">
            <div class="interactiveHeader">
              <h3 class="h3">Interactive marker comparison</h3>
              <p class="muted">
                Select a dataset and marker variant to see how accuracy and rankings change compared to the default.
              </p>
            </div>

            <div class="datasetSelector" style="margin-bottom: 1rem;">
              <label for="datasetSelect" style="font-weight: 600; margin-right: 0.5rem;">Dataset:</label>
              <select id="datasetSelect" class="datasetSelect" style="padding: 0.5rem; border: 1px solid #cbd5e1; border-radius: 0.375rem; font-size: 0.9rem;">
                <option value="DA2k" selected>DA2k</option>
                <option value="SPair">SPair</option>
              </select>
            </div>

            <div class="markerSelector">
              <button class="markerBtn markerBtn--active" data-marker="color_blue">Color: Blue</button>
              <button class="markerBtn" data-marker="marker_square">Marker Type: Square</button>
              <button class="markerBtn" data-marker="radius_3">Radius: 3</button>
              <button class="markerBtn" data-marker="text_offset_below">Text Offset: Below</button>
              <button class="markerBtn" data-marker="font_scale_0.2">Font Scale: 0.2</button>
            </div>

            <div class="interactiveGrid">
              <div class="chartPanel">
                <div id="accuracyChart" class="plotlyChart"></div>
              </div>
              <div class="leaderboardPanel">
                <table class="interactiveLb" id="interactiveLbTable">
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--center">Δ Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody id="interactiveLbBody">
                    <!-- Populated by JS -->
                  </tbody>
                </table>
              </div>
            </div>

            <p class="muted lbCaption">
              <strong>Small marker changes cause large, model-specific accuracy shifts and rank shuffles.</strong>
              Select a dataset and marker above to compare with the default evaluation.
            </p>
          </div>

          <h3 class="h2" style="margin-top: 3rem; margin-bottom: 1rem;">Low level implementation details matter</h3>
          <p class="muted" style="line-height: 1.6; font-size: 16px; margin-bottom: 1.5rem;">
            Even ostensibly irrelevant implementation details can significantly impact model performance on visually prompted tasks. We find that JPEG compression quality—a factor imperceptible to human evaluators—produces statistically significant changes in accuracy and rankings. Importantly, these compression effects are specific to visually prompted benchmarks; applying the same intervention to traditional VLM benchmarks like MMBench shows negligible impact, highlighting the unique fragility of visual prompting evaluations.
          </p>

          <div class="chartPanel" style="width: 100%; margin-left: auto; margin-right: auto; margin-bottom: 2rem;">
            <img src="assets/figures/jpeg_compression.png" alt="JPEG compression quality effects on model rankings" style="width: 100%; height: auto;">
            <p class="muted lbCaption" style="margin-top: 10px;">
              <strong>JPEG Compression can change ranks.</strong> JPEG compression quality can reorder rankings on visually prompted tasks (BLINK RD),
              while rankings on traditional benchmarks (MME) remain comparatively stable.
            </p>
          </div>

          <div class="lbSection" aria-label="Gaming benchmarks by changing markers">
            <div class="interactiveHeader">
              <h3 class="h3">Gaming benchmarks by changing markers</h3>
              <p class="muted">
                We demonstrate that visually prompted leaderboards can be effectively "gamed" by cherry-picking marker styles that favor specific models. For example, simply increasing the font size of a visual marker allowed us to artificially lift a weaker model, InternVL3-8B, to rank higher than the much larger Gemini 2.5 Pro.
              </p>
            </div>

            <div class="lbGrid" role="region" aria-label="Leaderboards under marker variants">
              <section class="lbCard" aria-label="Default leaderboard">
                <div class="lbHead">
                  <div class="lbTitle">Default</div>
                  <div class="lbSub">Standard Evaluation</div>
                </div>
                <table class="interactiveLb">
                  <colgroup>
                    <col class="lbColModel" />
                    <col class="lbColRank" />
                    <col class="lbColScore" />
                  </colgroup>
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Llama 4 Scout</td>
                      <td class="lbCell--center"><span class="lbRank">#1</span></td>
                      <td class="lbCell--right"><span class="lbScore">89.41%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Flash</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Pro</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr class="lbRow lbRow--highlight">
                      <td>InternVL3-<wbr />8B</td>
                      <td class="lbCell--center"><span class="lbRank">#4</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen3-VL-8B</td>
                      <td class="lbCell--center"><span class="lbRank">#4</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4.1</td>
                      <td class="lbCell--center"><span class="lbRank">#6</span></td>
                      <td class="lbCell--right"><span class="lbScore">75.29%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4o</td>
                      <td class="lbCell--center"><span class="lbRank">#7</span></td>
                      <td class="lbCell--right"><span class="lbScore">71.76%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen2.5-VL-7B</td>
                      <td class="lbCell--center"><span class="lbRank">#8</span></td>
                      <td class="lbCell--right"><span class="lbScore">70.59%</span></td>
                    </tr>
                    <tr>
                      <td>Gemma 3-4B</td>
                      <td class="lbCell--center"><span class="lbRank">#9</span></td>
                      <td class="lbCell--right"><span class="lbScore">52.94%</span></td>
                    </tr>
                  </tbody>
                </table>
              </section>

              <section class="lbCard" aria-label="Deflate InternVL3-8B leaderboard">
                <div class="lbHead">
                  <div class="lbTitle">Deflate InternVL3-8B</div>
                  <div class="lbSub">Marker Type Square</div>
                </div>
                <table class="interactiveLb">
                  <colgroup>
                    <col class="lbColModel" />
                    <col class="lbColRank" />
                    <col class="lbColScore" />
                  </colgroup>
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Llama 4 Scout</td>
                      <td class="lbCell--center"><span class="lbRank">#1</span></td>
                      <td class="lbCell--right"><span class="lbScore">90.59%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Pro</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">84.71%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen3-VL-8B</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4.1</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Flash</td>
                      <td class="lbCell--center"><span class="lbRank">#5</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4o</td>
                      <td class="lbCell--center"><span class="lbRank">#6</span></td>
                      <td class="lbCell--right"><span class="lbScore">75.29%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen2.5-VL-7B</td>
                      <td class="lbCell--center"><span class="lbRank">#7</span></td>
                      <td class="lbCell--right"><span class="lbScore">67.06%</span></td>
                    </tr>
                    <tr class="lbRow lbRow--highlight">
                      <td>InternVL3-<wbr />8B</td>
                      <td class="lbCell--center"><span class="lbRank">#8</span></td>
                      <td class="lbCell--right"><span class="lbScore">63.53%</span></td>
                    </tr>
                    <tr>
                      <td>Gemma 3-4B</td>
                      <td class="lbCell--center"><span class="lbRank">#9</span></td>
                      <td class="lbCell--right"><span class="lbScore">55.29%</span></td>
                    </tr>
                  </tbody>
                </table>
              </section>

              <section class="lbCard" aria-label="Inflate InternVL3-8B leaderboard">
                <div class="lbHead">
                  <div class="lbTitle">Inflate InternVL3-8B</div>
                  <div class="lbSub">Font Scale 1.0</div>
                </div>
                <table class="interactiveLb">
                  <colgroup>
                    <col class="lbColModel" />
                    <col class="lbColRank" />
                    <col class="lbColScore" />
                  </colgroup>
                  <thead>
                    <tr>
                      <th>Model</th>
                      <th class="lbCell--center">Rank</th>
                      <th class="lbCell--right">Score</th>
                    </tr>
                  </thead>
                  <tbody>
                    <tr>
                      <td>Llama 4 Scout</td>
                      <td class="lbCell--center"><span class="lbRank">#1</span></td>
                      <td class="lbCell--right"><span class="lbScore">85.88%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Flash</td>
                      <td class="lbCell--center"><span class="lbRank">#2</span></td>
                      <td class="lbCell--right"><span class="lbScore">81.18%</span></td>
                    </tr>
                    <tr class="lbRow lbRow--highlight">
                      <td>InternVL3-<wbr />8B</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen3-VL-8B</td>
                      <td class="lbCell--center"><span class="lbRank">#3</span></td>
                      <td class="lbCell--right"><span class="lbScore">77.65%</span></td>
                    </tr>
                    <tr>
                      <td>Gemini 2.5 Pro</td>
                      <td class="lbCell--center"><span class="lbRank">#5</span></td>
                      <td class="lbCell--right"><span class="lbScore">76.47%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4o</td>
                      <td class="lbCell--center"><span class="lbRank">#6</span></td>
                      <td class="lbCell--right"><span class="lbScore">72.94%</span></td>
                    </tr>
                    <tr>
                      <td>GPT-4.1</td>
                      <td class="lbCell--center"><span class="lbRank">#7</span></td>
                      <td class="lbCell--right"><span class="lbScore">71.76%</span></td>
                    </tr>
                    <tr>
                      <td>Qwen2.5-VL-7B</td>
                      <td class="lbCell--center"><span class="lbRank">#8</span></td>
                      <td class="lbCell--right"><span class="lbScore">70.59%</span></td>
                    </tr>
                    <tr>
                      <td>Gemma 3-4B</td>
                      <td class="lbCell--center"><span class="lbRank">#9</span></td>
                      <td class="lbCell--right"><span class="lbScore">49.41%</span></td>
                    </tr>
                  </tbody>
                </table>
              </section>
            </div>

            <p class="muted lbCaption">
              <strong>Leaderboards can be manipulated.</strong> Performance comparison — optimizing for InternVL3-8B's ranking on
              BLINK relative depth by changing the visual marker.
            </p>
          </div>
        </div>
      </section>

      <section class="section" id="conclusion">
        <div class="container">
          <h2 class="h2">Conclusion</h2>
          <p class="muted" style="line-height: 1.6; font-size: 16px;">
            Benchmarks should measure ability, not fragility. Yet our results reveal a gap: visually prompted evaluations
            <span aria-label="opening quote">&ldquo;</span>fall short of this standard<span aria-label="closing quote">&rdquo;</span>:
            change a marker’s color, shift its label, compress an image differently, and entire leaderboards reshuffle. These shifts are not noise but
            structural weaknesses, revealing that today's perception-focused VLM benchmarks are far more sensitive than the field assumes.
          </p>
          <p class="muted" style="line-height: 1.6; font-size: 16px; margin-top: 1rem;">
            Although our demonstrations center on BLINK-style tasks, the pattern is unlikely to be isolated. Any benchmark that depends on explicit
            visual markup or fine-grained spatial cues risks similar instability. If leaderboards can be flipped by choices orthogonal to task semantics,
            they cannot be trusted to track genuine progress. To address this, we recommend evaluations should diversify visual prompts, report variance
            in addition to scores, and standardize low-level settings that silently influence results. Our expanded <strong>VPBench</strong> is a step in
            that direction, offering larger, marker-diverse test sets that reduce incidental variance. Stable measurement is a prerequisite for meaningful
            comparison; until then, visually prompted leaderboards may be telling us more about their construction than about the models they rank.
          </p>
        </div>
      </section>

      <section class="section" id="citation">
        <div class="container">
          <h2 class="h2">Citation</h2>

          <div class="bibBox">
            <div class="bibBox__header">
              <div class="bibBox__label">BibTeX</div>
              <button class="btn btn--small" type="button" data-copy-target="#bibtex">
                Copy
              </button>
            </div>
            <pre class="bibtex" id="bibtex" style="white-space: pre-wrap; word-wrap: break-word; overflow-wrap: break-word;"><code>@inproceedings{vptblink2026fragile,
  title     = {Visually Prompted Benchmarks Are Surprisingly Fragile},
  author    = {Haiwen Feng and Long Lian and Lisa Dunlap and Jiahao Shu and XuDong Wang and Renhao Wang and Trevor Darrell and Alane Suhr and Angjoo Kanazawa},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2026},
  note      = {Submission},
}</code></pre>
          </div>
        </div>
      </section>
    </main>

    <script src="script.js"></script>
  </body>
</html>



